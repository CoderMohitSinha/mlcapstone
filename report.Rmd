---
title: 'Air quality prediction'
author: "Ger Inberg"
output: 
  pdf_document:
    latex_engine: xelatex
bibliography: references.bib
knit: (function(inputFile, encoding) { rmarkdown::render(inputFile, encoding=encoding, output_file='report.pdf') })
---
## Machine Learning Engineer Nanodegree - Capstone Project
Ger Inberg
May 5th, 2017

## I. Definition

### Project Overview

While travelling in South East Asia, I noticed the air quality issues in some bigger cities. It affects peoples lives directly because they might get breathing problems, will stay only inside buildings and/or are wearing masks. When people will know that at a certain time the air quality is bad, they can take measures to prevent possible (health) problems. Because I am curious about the current air quality prediction systems and if it can be improved I have chosen this as my subject.

In the "Human Health Effects on Air Pollution" study [@HealthEffects] the relation between air quality and the health of the people having to deal with that air have been shown. This has led to the introduction of the [@AirQualityIndex]. The AQI is an index for reporting daily air quality. It tells how clean or polluted the air is, and what might be the associated health effects. 

![Air Quality Index table](index.png)

The EPAâ€™s Air Quality Index is used daily by people suffering from asthma and other respiratory diseases to avoid dangerous levels of outdoor air pollutants, which can trigger attacks. There are already some systems that can predict air quality, however I would like to see if a more accurate model can be build.

The model I build could be used as the basis for an early warning system that is capable of accurately predicting dangerous levels of air pollutants on an hourly basis.
In the Air Quality Prediction Kaggle competition [@AirQualityHackathlon] this has been done already. However since this competition is already 5 years old, I want to use new techniques (for example XGBoost) to see if I can improve upon this.

### Problem Statement

Certain health problems are related to the air quality index. In order to prevent health issues due to bad air quality it is important to have an accurate estimate of it. When dangerous levels are reached, certain preventive measures can be taken, like to stay inside in the house. 

The best solution would be to make the air cleaner for example by less pollution. However that is not a solution that can be reached within in short time frame. However what we can do is to create awareness about the air quality and to signal when the level gets dangerous. People can act upon this signal by taking preventive measures and so some health problems can be prevented.

For every pollutant in the training data (see next section), the air quality level is expressed by a number, where a higher number means more pollution / worse air quality.
The algorithm has to predict the numeric value of the air quality for each pollutant. Thefore the algorithm has to solve a regression task.

There are multiple machine learning algorithms that can solve a regression task. The eXtreme Gradient Boosting (xgboost) is nowadays a very popular algorithm and wins many competitions on Kaggle [@XGBoostWinningSolutions].
Therefore I will be using xgboost.

### Metrics

To measure the performance of a regression model, multiple metrics can be used, such as Root Mean Squared Error (RMSE) and the Mean Absolute Error (MAE).

The MAE is a quantity used to measure how close forecasts or predictions are to the eventual outcomes. It is defined by the average of the absolute differences. 
The RMSE also takes into account the difference between predictions and eventual outcomes, however it severely punishes large errors.
The metrics can be calculated in R as follow:

```{r, eval=FALSE}
MAE <- sum(abs(y-y_pred)) / length(y)
RMSE <- sqrt(mean((y-y_pred)^2))
```

To determine which metric to use, it is important to think about what a good model is.
First, a good model should have predictions that are close to the eventual outcomes. Second, it should take into account the distance between the predicted values and the outcomes.
Both of these metrics do this. 
If we have 2 predictions and the 2nd one is twice as big as the first one, the contribution of the 2nd one to MAE is just twice as big as the first one. For RMSE, the contribution of the 2nd forecast will be four time as much as the 1st forecast, because of the square.
It is not needed to severly punish for large errors, that's why I will use the MAE.

## II. Analysis

### Data Exploration

The datasets that I will use are provided by Kaggle, a description of the data can be found on [their data page] (https://www.kaggle.com/c/dsg-hackathon/data).
The datasets consists of SiteLocation data, TrainingData and a sample submission file. The sample submission file contains the test data.

The training data consists of 37821 rows while the test data consists of 2100 rows. So, the test data is only about 5.5% of the training data, which means there is relatively a large amount of training data.

The training data consists of the following features:

* rowID
* chunkID
* position_within_chunk (starts at 1 for each chunk of data, increments by hour)
* month_most_common (most common month within chunk of data--a number from 1 to 12) weekday (day of the week, as a string)
* hour (a number from 0 to 23, local time)
* Solar.radiation_64 
* WindDirection..Resultant_1 (direction the wind is blowing from given as an angle, e.g. a wind from the east is "90")
* WindDirection..Resultant_1018 (direction the wind is blowing from given as an angle, e.g. a wind from the east is "90")
* WindSpeed..Resultant_1 ("1" is site number)
* WindSpeed..Resultant_1018 ("1018" is site number)
* Ambient.Max.Temperature_(site number)
* Ambient.Min.Temperature_(site number)
* Sample.Baro.Pressure_(site number)
* Sample.Max.Baro.Pressure_(site number)
* Sample.Min.Baro.Pressure_(site number)
* (39 response variables of the form): target_(target number)_(site number) 

The definition of the response variables:  

* Target_1    EC CSN PM2.5 LC TOT
* Target_2    Total Nitrate PM2.5 LC 
* Target_3    SO2 max 5-min avg 
* Target_4    Sulfur dioxide
* Target_5    PM10 Total 0-10um STP    
* Target_7    Total Carbon PM2.5 LC TOT 
* Target_8    Sulfate PM2.5 LC 
* Target_9    Oxides of nitrogen (NOx) 
* Target_10   Nitric oxide (NO)
* Target_11   Ozone
* Target_14   Nitrogen dioxide (NO2)
* Target_15   OC CSN Unadjusted PM2.5 LC TOT

As can be seen, there are 39 output/response variables that have to be predicted. The other features are input features.
However the testing dataset only contains the first 5 features, so only these features should be used in training the algorithm. It doesn't make sense to use the other input features since, they cannot be used when evaluating the algorithm. These 5 features (rowId, chunkID, position_within_chunk, month_most_common and hour) are all categorical features.

These datasets are relevant since they contain hourly data about locations and of various quantities including pollutants. With these features a model can be created that predicts the airquality for a given location and time of day. With this prediction, it can be determined if the level is dangerous or not (and thus if a warning should be triggered).

### Exploratory Visualization

Since the test data contains only the first 5 features, I have decided to focus on the relation between these features and the targets. There are many observations in the data, so just plotting the feature data against the targets will probably not give a plot that gives much insight. However each feature contains less unique values, so if we group by a feature and average the targets, there will be less datapoints. For example, there are many chunks and most of them have common 'hour' values since hour ranges from 0-23.

When looking at the plot between hour and targets, there seems to be repetitive pattern (sinus function) for most targets. I would also expect this, since at certain times of the day there is more traffic and factories might produce more pollution which affects certain pollutants. 

![Average pollution per hour](images/mean_targets_per_hour.png)
Below the plot is drawn between position_in_chunk and targets, there also seems to be some kind of relation for most targets. 

![Average pollution per hour](images/mean_targets_per_chunk_pos.png)

I have also made these plots for the other features (see notebook), however there didn't seem to be a relation between those features and targets.

### Algorithms and Techniques

As mentioned in my proposal I will use the eXtreme Gradient Boosting (XGBoost) library to create a model for my predictions. XGBoost is a very popular library that is used successfully in a lot of Kaggle competitions.
The reasons why I want to use it for this project:

* it is fast, because it automatically parallels execution of the code
* it is accurate
* it can be used for a regression problem
* it has advanced features to tweak a model and get a better score

For a technical explanation about the model, please see [@XGboost]

XGBoost requires that the input data is all numeric. Therefore, some preprocessing needs to be done, which I will disucss in Data Preprocessing.

### Benchmark

Each kaggle competition contains a leaderbord with scores of the participants. Next to these scores, some benchamrk scores are provided. For this competition the next benchmarks and scores are provided

* predicting using average by hour over chunk (0.27532)
* predict using hourly averages               (0.29362)
* SubmissionZerosExceptNAs.csv                (0.53541)
* SubmissionAllZerosEvenNAsVeryBadScore.csv   (517253.56661)

Because the score is the Mean Absolute Error (MAE), obviously a lower score is better.
The last 2 benchmarks are clearly too simple (given their name) so it's not a serious benchmark to consider. The first one seems of a moderate complexity given it's name and it's position on the leaderboard. Therefore I will use this as my reference model.

## III. Methodology

### Data Preprocessing

Some features needed preprocessing in order for XGboost to process them. 

The target features contain numeric values but also some NA values. Since we can't train the model on a NA value, I decided to remove this row from the dataset.
The weekday feature contains character data, that XGBoost cannot handle. Therefore I have transformed this feature to numeric values where a Monday is a 0, Tuesday a 1, etc.

Categorical features should be transformed as well. The chunkID and month_most_common are definitely categorical in my opinion since there is no relation between the values in each of these features.

For the weekday, hour and position_within_chunk it is less easy to determine if these should be treated as categorical or not. For example: if there is a lot of pollution on Monday, does it influence the pollution on the next day(s)?
For hour and position_within_chunk I said in the section Exploratory Visualization that there seems to be a relation between the feature and the targets. So, if these features are transformed with one-hot encoding, the relation between the values is lost. Therefore I decided to treat hour and position_within_chunk as numeric features. Because the weekday didn't seem to have a relation with the targets, I treated it as a categorical feature. 

### Implementation

The first issue I stumbled upon is that XGBoost cannot handle multiple labels. Since there are 39 targets to be predicted, I had to find some other way in order to be be able this algorithm.
The second issue was that I could not compute a test score anymore on the Kaggle website, since the competition is already closed for a while.

To tackle these issues, I decided to create a model for each target feature, so 39 models. Each model uses the TrainingData dataset and splits this up in a training and test test.
In this way, the testing score can be calculated for each model. The final testing score can be determined by averaging the test scores of all models.
This approach has the disadvantage that is costs more CPU time, but because XGBoost is fast, it should still be possible to get results within a reasonable amount of time.

The steps that I am performing to train the data and calculate the testing score

* create a list of labels. For each label the next steps are done:

* split all the data in X (features) and y (label)
* remove all rows with NA for the label
* split the remaining data in train and test set

* one hot encoding categorical features in training data
* create XGB model 
* fit the XGB model on the training data

* one hot encoding categorical features in testing data
* calculate testing score

* calculate final testing score by avering testing score of all models

### Refinement

XGBoost has quite some parameters that can be used to improve the score. This website [@TuningXGBoost] has been really helpful to me in understanding how to improve the model.

I played around with a couple of parameters and especially the number of boosted rounds mattered a lot for the testing score.
For a value of 100 boosting rounds the score was 0.3996, while for 1000 rounds this was 0.2386. 

## IV. Results

### Model Evaluation and Validation

I have run my code multiple times to see if the scores were stable. In the final model these parameters are used:

* eta (learning rate): 0.1  
* seed (the random number seed): 21
* subsample : 0.0
* colsample_bytree : 0.0
* max_depth : 5
* min_child_weight : 1
* objective : reg:linear
* eval_metric : mae

It turned out that the test score is close to 0.24 all the time, so it's quite stable. This is caused by 2 factors. 
Firstly, I am using the train_test split procedure of sklearn to split the data in a training and a test set. These sets however are randomized, so each time this returns a different dataset for training and testing. This means that the training and testing is done on a different set each time. So if the model is only performing well on part of the dataset, I would expect the scores to change drastically each time.
Secondly, there are 39 models and the final score is calculated by taking the average of these models. So by averaging these results, the model will be more generally applicable. 

### Justification

My final result test score is 0.2382, which is better than then the benchmark score of 0.2753. The improvement is about 13.5 %.
Since the score is better, the predictions are closer to the real values. This means that the foundation for an early warning system is improved and a more accurate warning can be given. 

## V. Conclusion

### Free-Form Visualization

I have created a couple of plots from my final model. Since there are 2 that are both interesting I choose to add them both.
The first one visualizes the feature importances for the last model ordered in descending order. The position_within_chunk is the most important feature followed by hour and then some chunkID's and weekdays.
It is quite surprising to me that the position_within_chunk is more important than hour.

![Average pollution per hour](images/feature_importances.png)

The next plot gives the test scores for each model in ascending order. Thus the model with the best score is the model related to 'target_3_50'.
One can see that the range of scores is quite large, ranging from 0.05 to 0.40.

![Average pollution per hour](images/test_scores.png)

### Reflection

It took me quite a while to find a subject for this project. Since there are so many different domains where machine learning can be applied and I have a broad interest. However, I started thinking about air pollution when I was travelling in SE Asia and noticed it myself. When I saw that people are wearing masks and I read about health issues related to the air pollution, I realized it is a big problem in certain areas.
I found out there has been a Kaggle competition related to it, which was very helpful. The capstone project was quite overwhelming at first, because you have to do a lot more than in the other projects of the Nanodegree. Being able to use the Kaggle project was a big help, since I could use the dataset and there was some sample code to get started. 
Since I noticed the popularity of XGBoost, I decided that I wanted to use this algorithm. Since it is good to gain knowledge of such a popular algorithm and I wanted to see myself how it performs.

I found out that XGBoost does not support multi label classification at the moment. So, my biggest struggle has been to find out how to solve this. When doing an online search, I found out that more people have run into this and some just created a model for each label. So, I decided to follow this approach as well.
Another issue was that the testing score could not be calculated anymore on Kaggle. I could have used the training score as well since there is a public and a private leaderbord where the public one contains the training scores.
However, since the dataset is quite big, I decided to use part of the training set as test data.

The most interesting things I have learned about this project are about the algorithm and the most important features relating to the labels.
XGBoost is really fast, it's very convenient that it uses multiple cores by default so you don't have to think about that yourself. Furthermore, it gives a good score out of the box, so wiithout having to put a lot of effort in parameter tuning. I am definitely planning to use this algorithm more often!

### Improvement

I haven't used gridSearch to find the optimal parameters. I tried it, but it took a long time to calculate it even for 1 model, let alone for 39 models. Luckily the score was already quite good, by tuning some parameters myself such as the number of boosting rounds. However, I am quite sure a gridSearch to find the optimal parameters will deliver an even improved score.

I am not sure if the XGBoost approach I have followed for multi label regression is the most efficient. I have found a way that is working for this dataset and the result is good, but I didn't have the time to look if this is the most optimal approach. It would be worth to investigate if there is a way to do multi label regression with less computation time and/or less manual code.

### References
